# EmbodiedAnalysis

The Embodied Analysis repository is a tool designed to streamline the statistical analysis and visualization of data generated by the Embodied Pipeline. This repository contains a set of scripts that facilitate the processing of directories containing CSV files, allowing users to compile data, perform statistical analyses, and create informative visualizations.

## Table of Contents

- [Overview](#overview)
- [Scripts](#scripts)
- [Directory Structure](#directory-structure)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
- [Future Improvements](#future-improvements)

## Overview

The purpose of this repository is to simplify the analysis of data generated by the Embodied Pipeline. It assumes that the input data is organized in directories, with each directory representing a different brain architecture. The scripts in this repository can handle data with specific naming conventions:

1. Each CSV filename ends with either "train.csv" or "exp.csv."
2. The imprinting condition is specified at the beginning of the filename, followed by a dash ("-").
3. There are no other dashes in the CSV filename.
4. The agent ID number is the only number in the CSV filename.

## Scripts

There are three main scripts in this repository:
### 1. NETT_merge_csvs.R
This script compiles all the CSV files in a specified directory into a single data file. Optionally, it can produce separate training and testing results CSV files.

### 2. NETT_train_viz.R
This script uses the output data file from NETT_merge_csvs.R. It produces visualizations of training performance at both the individual and imprinting condition levels.

### 3. NETT_test_viz.R
This script also uses the output data file from NETT_merge_csvs.R. It generates statistics tables and graphs of performance by test condition at the individual, imprinting condition, and fully collapsed levels.

To run these scripts, you need to have R installed, along with the R libraries tidyverse, scales, and argparse.

## Directory Structure

To use these scripts effectively, your directory structure should adhere to the following format:

- Parent Directory (Root)
  - Brain Architecture 1
      - filename1-train.csv
      - filename2-exp.csv
      ...
  - Brain Architecture 2
      - filename3-train.csv
      - filename4-exp.csv
      ...
  ...

  You will need to run the scripts separately for each Brain Architecture directory.

## Getting Started

### Prerequisites
Before using the scripts, ensure that you have the following prerequisites installed:

R (https://www.r-project.org/)
R libraries: tidyverse, scales, argparse

### Installation
1. Clone this repository to your local machine.
2. Install the required R libraries if you haven't already:
```

install.packages("tidyverse")
install.packages("scales")
install.packages("argparse")

```

## Usage

You can run all these scripts from the command line. Here are examples of how to use them:

`Rscript NETT_merge_csvs.R --logs-dir /path/to/logs_directory --results-dir /path/to/results_directory --results-name output_file.R --csv-test test_output.csv --csv-train train_output.csv`

`Rscript NETT_train_viz.R --data-loc /path/to/results_directory/output_file.R --results-wd /path/to/results_directory/ --ep-bucket 100`

`Rscript NETT_test_viz.R --data-loc /path/to/results_directory/output_file.R --key-csv /path/to/key.csv --results-wd /path/to/results_directory/ --color-dots true`

Remember to replace the placeholder paths with the actual paths to your data and output directories.

## Future Improvements

In future versions of this repository, we plan to remove the requirement for a key file to further simplify the usage of these scripts.
