{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb94be2-d347-45d0-bcc0-dba106786b65",
   "metadata": {},
   "source": [
    "# Test LSTM Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "1afdda94-9d5e-4bba-9b41-883e34f75978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "import gym\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "\n",
    "class CnnLSTMEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    :param observation_space: (gym.Space)\n",
    "    :param features_dim: (int) Number of features extracted.\n",
    "        This corresponds to the number of unit for the last layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, observation_space: gym.spaces.Box, \n",
    "                  features_dim: int = 256):\n",
    "        \n",
    "        rnn_hidden_size = 100\n",
    "        rnn_num_layers = 1\n",
    "        super().__init__()\n",
    "        \n",
    "        # We assume CxHxW images (channels first)\n",
    "        # Re-ordering will be done by pre-preprocessing or wrapper\n",
    "        n_input_channels = observation_space.shape[0]\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(n_input_channels, 32, kernel_size=8, stride=4, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        # Compute shape by doing one forward pass\n",
    "        with th.no_grad():\n",
    "            n_flatten = self.cnn(th.as_tensor(observation_space.sample()[None]).float()).shape[1]\n",
    "\n",
    "        self.rnn = nn.LSTM(n_flatten, rnn_hidden_size, rnn_num_layers)\n",
    "        self.linear = nn.Sequential(nn.Linear(rnn_hidden_size, features_dim), nn.ReLU())\n",
    "        \n",
    "    def forward(self, observations: th.Tensor) :\n",
    "        print(observations.shape, observations.dtype)\n",
    "        b_z, ts, c, h, w = observations.shape\n",
    "        ii = 0\n",
    "        y = self.cnn((observations[:,ii]))\n",
    "        \n",
    "        out, (hn, cn) = self.rnn(y.unsqueeze(1))\n",
    "        out = self.linear(out) \n",
    "        \n",
    "        return out \n",
    "\n",
    "    \n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "6c30c58f-4398-435c-b043-5e3b861df3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test encoder\n",
    "from src.simulation.env_wrapper.parsing_env_wrapper import ParsingEnv\n",
    "from hydra import initialize, compose\n",
    "from omegaconf import DictConfig, OmegaConf, open_dict\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecTransposeImage\n",
    "\n",
    "import os\n",
    "def test_encoder(encoder_cls, device):\n",
    "    num_envs = 3\n",
    "    with initialize(version_base=None, config_path=\"src/simulation/conf\"):\n",
    "        cfg = compose(config_name=\"config\")\n",
    "\n",
    "    env_config = cfg[\"Environment\"]\n",
    "    with open_dict(env_config):\n",
    "            object =  \"ship\" if env_config[\"use_ship\"] else \"fork\"\n",
    "            env_config[\"mode\"] = \"rest\" + \"-\"+ object +\"-\"+env_config[\"background\"]\n",
    "            env_config[\"random_pos\"] = True\n",
    "            env_config[\"rewarded\"] = True\n",
    "            env_config[\"run_id\"] = cfg[\"run_id\"] + \"_\" + \"test\"\n",
    "            env_config[\"rec_path\"] = os.path.join(env_config[\"rec_path\"] , f\"agent_0/\")   \n",
    "    env = ParsingEnv(**env_config)\n",
    "    e_gen = lambda : env\n",
    "    train_env = make_vec_env(env_id=e_gen, n_envs=1)\n",
    "    train_env = VecTransposeImage(train_env)\n",
    "        \n",
    "\n",
    "    \n",
    "    device = th.device(device)\n",
    "    encoder = encoder_cls(observation_space=train_env.observation_space, features_dim=50).to(device)\n",
    "    time_step = train_env.reset()\n",
    "    time_step = time_step.reshape(time_step.shape[0],1,time_step.shape[1],time_step.shape[2],time_step.shape[3])\n",
    "    encoder(th.from_numpy(time_step).type(th.FloatTensor).to(device))\n",
    "    \n",
    "    print(\"Encoder test passed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5d97e10c-8dfa-4ce6-888d-96c6c4fce936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Connected to Unity environment with package version 2.0.1 and communication version 1.5.0\n",
      "[INFO] Connected new brain: ChickAgent?team=0\n",
      "0.26.2\n",
      "[WARNING] Could not seed environment ChickAgent?team=0\n",
      "torch.Size([1, 1, 3, 64, 64]) torch.float32\n",
      "Encoder test passed!\n"
     ]
    }
   ],
   "source": [
    "test_encoder(CnnLSTMEncoder,\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72bc01e-f52e-4451-913d-e11353cd8cde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chickai_benchmark_env",
   "language": "python",
   "name": "chickai_benchmark_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
